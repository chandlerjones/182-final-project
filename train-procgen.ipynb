{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"train-procgen.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"gwZQVzDGKWll","colab_type":"code","outputId":"35302d38-73f7-45bd-c96c-aae47b6baec5","executionInfo":{"status":"ok","timestamp":1589217481771,"user_tz":420,"elapsed":21019,"user":{"displayName":"Chandler Jones","photoUrl":"","userId":"10095419221959852274"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dsfBhsJUlQa3","colab_type":"code","outputId":"11e8989b-bd78-4d15-aaf9-32015e0b9dfa","executionInfo":{"status":"ok","timestamp":1589228912730,"user_tz":420,"elapsed":231,"user":{"displayName":"Chandler Jones","photoUrl":"","userId":"10095419221959852274"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["cd 'drive/My Drive/final/'"],"execution_count":1,"outputs":[{"output_type":"stream","text":["/content/drive/.shortcut-targets-by-id/1LlyPSYhxPjZ3mmfbs80U9Ni-AoklGoNU/final\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pfSeP7aBkzAC","colab_type":"code","colab":{}},"source":["# Uncomment only if you do not already have all of the dependancies downloaded. \n","\n","#!git clone https://github.com/openai/train-procgen.git\n","!pip install https://github.com/openai/baselines/archive/9ee399f5b20cd70ac0a871927a6cf043b478193f.zip\n","!pip install -e train-procgen\n","\n","#!git clone https://github.com/openai/baselines.git\n","!pip install baselines"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jf24ZwPxlynx","colab_type":"code","outputId":"d79817f9-9b7d-4f00-beb4-15c7a463dc6c","executionInfo":{"status":"ok","timestamp":1589155831447,"user_tz":420,"elapsed":277,"user":{"displayName":"Andrew Jones","photoUrl":"","userId":"16423324799626053004"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["cd 'train-procgen'"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Colab Notebooks/final/train-procgen\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XJ96WV0al5Vg","colab_type":"code","colab":{}},"source":["!pip install procgen\n","!pip install mpi4py\n","!pip install gym"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SH5QsK2o961K","colab_type":"code","colab":{}},"source":["%load_ext autoreload\n","%autoreload 2\n","!pip install --upgrade tensorflow-gpu==1.15.0"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jhw7k6yLpSqF","colab_type":"code","outputId":"f07644a9-f1f2-4682-eea2-50f70388a5fc","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["\"\"\"\n","!!!WARNING!!! --> If using Jupyter Notebook, Google Colab, or any other iPython notebook client, you must restart the kernel (runtime)\n","before running this cell again. The implementation set forth by OpenAI does not close the tensorflow session and clear the computation graph,\n","thus when you try to call this function again, the variables have already been allocated and tensorflow gets mad. I have tried going into the source\n","code and clearing the graph at the end of training, but I still encountered this error. The most reliable and least painful fix is to just restart the\n","kernel if you plan on training again.\n","\n","Taken from https://github.com/openai/train-procgen.\n","Default is to use ppo2. Here we modify to use dqn instead of ppo.\n","\n","TRAIN-PROCGEN using DEEP Q-NETWORK\n","implementation of DQN courtesy of OpenAI: https://github.com/openai/baselines/blob/master/baselines/deepq/deepq.py \n","Hyperparameters: see defaults in cell above\n","\"\"\"\n","\n","import tensorflow as tf\n","import gym\n","from baselines.ppo2 import ppo2\n","from baselines.deepq import deepq\n","from baselines.common.models import build_impala_cnn\n","from baselines.common.mpi_util import setup_mpi_gpus\n","from procgen import ProcgenEnv\n","from baselines.common.vec_env import (\n","    VecExtractDictObs,\n","    VecMonitor,\n","    VecFrameStack,\n","    VecNormalize\n",")\n","from baselines import logger\n","from mpi4py import MPI\n","import argparse\n","\n","LOG_DIR = 'log/procgen'\n","\n","env_name='procgen:procgen-fruitbot-v0'\n","start_level=0\n","distribution_mode='easy'\n","\n","total_timesteps=50000\n","num_levels=100\n","buffer_size=10000\n","test_worker_interval = 0\n","\n","gamma=0.999\n","learning_rate=5e-4\n","\n","comm = MPI.COMM_WORLD\n","rank = comm.Get_rank()\n","\n","is_test_worker = False\n","\n","if test_worker_interval > 0:\n","    is_test_worker = comm.Get_rank() % test_worker_interval == (test_worker_interval - 1)\n","\n","mpi_rank_weight = 0 if is_test_worker else 1\n","num_levels = 0 if is_test_worker else num_levels\n","\n","log_comm = comm.Split(1 if is_test_worker else 0, 0)\n","format_strs = ['csv', 'stdout'] if log_comm.Get_rank() == 0 else []\n","logger.configure(dir=LOG_DIR, format_strs=format_strs)\n","\n","logger.info(\"creating environment\")\n","\n","env = gym.make(env_name, start_level=start_level, num_levels=num_levels, distribution_mode=distribution_mode)\n","\n","logger.info(\"creating tf session\")\n","setup_mpi_gpus()\n","config = tf.ConfigProto()\n","config.gpu_options.allow_growth = True #pylint: disable=E1101\n","sess = tf.Session(config=config)\n","sess.__enter__()\n","\n","conv_fn = lambda x: build_impala_cnn(x, depths=[16,32,32], emb_size=256)\n","\n","logger.info(\"training\")\n","model = deepq.learn(\n","    env,\n","    network=conv_fn,\n","    seed=420,\n","    dueling=False,\n","    lr=5e-4,\n","    checkpoint_path=\"fruitbot_checkpoint\",\n","    total_timesteps=2000000,\n","    print_freq=100,\n","    buffer_size=50000,\n","    exploration_fraction=0.2,\n","    exploration_final_eps=0.05,\n","    learning_starts=100000,\n","    target_network_update_freq=1000,\n","    gamma=0.99999,\n","    prioritized_replay=True,\n","    param_noise=False\n",")\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:\n","The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","  * https://github.com/tensorflow/io (for I/O related ops)\n","If you depend on functionality not listed there, please file an issue.\n","\n","Logging to log/procgen\n","creating environment\n","creating tf session\n","training\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/baselines/common/tf_util.py:53: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/baselines/common/misc_util.py:58: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/baselines/deepq/deepq.py:205: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/baselines/deepq/build_graph.py:238: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/baselines/common/input.py:31: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/baselines/common/input.py:57: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.cast` instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/baselines/deepq/build_graph.py:246: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/baselines/common/models.py:43: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.keras.layers.Conv2D` instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `layer.__call__` method instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/baselines/common/models.py:57: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.MaxPooling2D instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/baselines/common/models.py:67: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.flatten instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/baselines/common/models.py:69: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.Dense instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/baselines/deepq/build_graph.py:123: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/baselines/deepq/build_graph.py:115: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/baselines/deepq/build_graph.py:116: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/baselines/deepq/build_graph.py:132: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/baselines/deepq/build_graph.py:269: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/baselines/deepq/build_graph.py:266: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/baselines/deepq/build_graph.py:279: The name tf.log is deprecated. Please use tf.math.log instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/baselines/deepq/build_graph.py:296: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/baselines/deepq/build_graph.py:298: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/baselines/common/tf_util.py:89: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/baselines/common/tf_util.py:90: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n","  out=out, **kwargs)\n","/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n","  ret = ret.dtype.type(ret / rcount)\n"],"name":"stderr"},{"output_type":"stream","text":["--------------------------------------\n","| % time spent exploring  | 97       |\n","| episodes                | 100      |\n","| mean 100 episode reward | -2.6     |\n","| steps                   | 1.02e+04 |\n","--------------------------------------\n","--------------------------------------\n","| % time spent exploring  | 95       |\n","| episodes                | 200      |\n","| mean 100 episode reward | -2.2     |\n","| steps                   | 1.99e+04 |\n","--------------------------------------\n","--------------------------------------\n","| % time spent exploring  | 92       |\n","| episodes                | 300      |\n","| mean 100 episode reward | -2.4     |\n","| steps                   | 2.95e+04 |\n","--------------------------------------\n","--------------------------------------\n","| % time spent exploring  | 90       |\n","| episodes                | 400      |\n","| mean 100 episode reward | -2.2     |\n","| steps                   | 3.95e+04 |\n","--------------------------------------\n","--------------------------------------\n","| % time spent exploring  | 88       |\n","| episodes                | 500      |\n","| mean 100 episode reward | -1.6     |\n","| steps                   | 4.86e+04 |\n","--------------------------------------\n","--------------------------------------\n","| % time spent exploring  | 86       |\n","| episodes                | 600      |\n","| mean 100 episode reward | -2       |\n","| steps                   | 5.79e+04 |\n","--------------------------------------\n","--------------------------------------\n","| % time spent exploring  | 83       |\n","| episodes                | 700      |\n","| mean 100 episode reward | -1.9     |\n","| steps                   | 6.76e+04 |\n","--------------------------------------\n","--------------------------------------\n","| % time spent exploring  | 81       |\n","| episodes                | 800      |\n","| mean 100 episode reward | -1.8     |\n","| steps                   | 7.71e+04 |\n","--------------------------------------\n","--------------------------------------\n","| % time spent exploring  | 79       |\n","| episodes                | 900      |\n","| mean 100 episode reward | -2       |\n","| steps                   | 8.57e+04 |\n","--------------------------------------\n","--------------------------------------\n","| % time spent exploring  | 77       |\n","| episodes                | 1e+03    |\n","| mean 100 episode reward | -1.8     |\n","| steps                   | 9.49e+04 |\n","--------------------------------------\n","--------------------------------------\n","| % time spent exploring  | 75       |\n","| episodes                | 1.1e+03  |\n","| mean 100 episode reward | -2.4     |\n","| steps                   | 1.05e+05 |\n","--------------------------------------\n","Saving model due to mean reward increase: None -> -2.1\n","--------------------------------------\n","| % time spent exploring  | 72       |\n","| episodes                | 1.2e+03  |\n","| mean 100 episode reward | -2.3     |\n","| steps                   | 1.15e+05 |\n","--------------------------------------\n","--------------------------------------\n","| % time spent exploring  | 70       |\n","| episodes                | 1.3e+03  |\n","| mean 100 episode reward | -1.6     |\n","| steps                   | 1.23e+05 |\n","--------------------------------------\n","Saving model due to mean reward increase: -2.1 -> -1.2\n","--------------------------------------\n","| % time spent exploring  | 68       |\n","| episodes                | 1.4e+03  |\n","| mean 100 episode reward | -1.2     |\n","| steps                   | 1.32e+05 |\n","--------------------------------------\n","--------------------------------------\n","| % time spent exploring  | 66       |\n","| episodes                | 1.5e+03  |\n","| mean 100 episode reward | -1.5     |\n","| steps                   | 1.42e+05 |\n","--------------------------------------\n","--------------------------------------\n","| % time spent exploring  | 64       |\n","| episodes                | 1.6e+03  |\n","| mean 100 episode reward | -1.2     |\n","| steps                   | 1.5e+05  |\n","--------------------------------------\n","--------------------------------------\n","| % time spent exploring  | 62       |\n","| episodes                | 1.7e+03  |\n","| mean 100 episode reward | -1       |\n","| steps                   | 1.59e+05 |\n","--------------------------------------\n","Saving model due to mean reward increase: -1.2 -> -0.9\n","--------------------------------------\n","| % time spent exploring  | 60       |\n","| episodes                | 1.8e+03  |\n","| mean 100 episode reward | -1       |\n","| steps                   | 1.68e+05 |\n","--------------------------------------\n","--------------------------------------\n","| % time spent exploring  | 57       |\n","| episodes                | 1.9e+03  |\n","| mean 100 episode reward | -1.3     |\n","| steps                   | 1.78e+05 |\n","--------------------------------------\n","--------------------------------------\n","| % time spent exploring  | 55       |\n","| episodes                | 2e+03    |\n","| mean 100 episode reward | -2       |\n","| steps                   | 1.88e+05 |\n","--------------------------------------\n","--------------------------------------\n","| % time spent exploring  | 53       |\n","| episodes                | 2.1e+03  |\n","| mean 100 episode reward | -1.8     |\n","| steps                   | 1.97e+05 |\n","--------------------------------------\n","--------------------------------------\n","| % time spent exploring  | 50       |\n","| episodes                | 2.2e+03  |\n","| mean 100 episode reward | -1.2     |\n","| steps                   | 2.07e+05 |\n","--------------------------------------\n","--------------------------------------\n","| % time spent exploring  | 47       |\n","| episodes                | 2.3e+03  |\n","| mean 100 episode reward | -1.6     |\n","| steps                   | 2.2e+05  |\n","--------------------------------------\n","--------------------------------------\n","| % time spent exploring  | 44       |\n","| episodes                | 2.4e+03  |\n","| mean 100 episode reward | -2       |\n","| steps                   | 2.33e+05 |\n","--------------------------------------\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"07Bz--UYXuri","colab_type":"code","colab":{}},"source":["conv_fn = lambda x: build_impala_cnn(x, depths=[16,32,32], emb_size=256)\n","model = deepq.learn(gym.make('procgen:procgen-fruitbot-v0'), \n","                    load_path='drive/My Drive/final/fruitbot_checkpoint',\n","                    network=conv_fn,\n","                    total_timesteps=1000)\n","while True:\n","        obs, done = env.reset(), False\n","        episode_rew = 0\n","        while not done:\n","            env.render()\n","            obs, rew, done, _ = env.step(model(obs[None])[0])\n","            episode_rew += rew\n","        print(\"Episode reward\", episode_rew)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oxvbwAYWli9n","colab_type":"code","colab":{}},"source":["!python -m train_procgen.train --env_name fruitbot # Uses the PPO2 default values in train-procgen. This is the baseline."],"execution_count":0,"outputs":[]}]}